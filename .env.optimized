# Phi4 MCP Service - OPTIMIZED Configuration
# This configuration reduces memory usage by 70-80%

# Server Configuration
PORT=3003
HOST=0.0.0.0
NODE_ENV=development

# Parser Configuration - OPTIMIZED
# Only enable ONE parser to reduce memory usage
DEFAULT_PARSER=distilbert
ENABLE_DISTILBERT=true
ENABLE_HYBRID=false    # ❌ DISABLED - Saves ~500MB
ENABLE_FAST=false      # ❌ DISABLED - Saves ~50MB (can enable as fallback if needed)
ENABLE_ORIGINAL=false  # ❌ DISABLED - Saves ~300MB
ENABLE_PHI4=true

# Model Configuration
MODEL_CACHE_DIR=./models
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1:latest

# Phi4 LLM Configuration (using Llama3.1 via Ollama)
ENABLE_PHI4=true
PHI4_API_URL=http://127.0.0.1:11434/api/generate
OLLAMA_MODEL=llama3.1
PHI4_CONTEXT_LENGTH=4096
PHI4_MAX_TOKENS=2048
PHI4_TEMPERATURE=0.7

# Embedding Models
EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2
NER_MODEL=Xenova/bert-base-multilingual-cased-ner-hrl

# API Keys
MCP_PHI4_API_KEY=your-api-key-here

# Security
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Rate Limiting
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX_REQUESTS=100

# Performance - OPTIMIZED
REQUEST_TIMEOUT=10000
MAX_CONCURRENT_REQUESTS=10
MODEL_WARMUP_ON_START=false  # ❌ DISABLED - Lazy load on first request instead

# Logging
LOG_LEVEL=info
LOG_FORMAT=json

# Memory Optimization
# These are Node.js flags you can pass when starting the service
# Example: NODE_OPTIONS="--max-old-space-size=512" yarn dev
# --max-old-space-size=512  # Limit heap to 512MB (default is 4GB)
# --optimize-for-size       # Optimize for memory, not speed

# OPTIMIZATION SUMMARY:
# ✅ Only DistilBERT parser enabled (best accuracy, ~500MB)
# ✅ Lazy loading (models load on first request, not at startup)
# ✅ Disabled warmup (saves 2-3 seconds startup time)
# ✅ Disabled unused parsers (saves ~850MB RAM)
#
# EXPECTED MEMORY USAGE:
# - Startup: ~100MB (just Express server)
# - After first request: ~600MB (DistilBERT loaded)
# - Savings: ~1.2GB compared to full configuration
